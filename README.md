# ğŸ§  FastAPI Chatbot with MongoDB & LLM Integration

This is a lightweight chatbot backend powered by **FastAPI**, **MongoDB**, and a Language Learning Model (LLM). The frontend is a simple HTML/JS app powered by **Gemini-generated UI**, and uses REST APIs to interact with the backend.

---

## ğŸ“ Project Structure  
â”œâ”€â”€ app/  
â”‚ â”œâ”€â”€ api/   # FastAPI route handlers 

â”‚ â”œâ”€â”€ config/ # Database and LLM configuration 

â”‚ â”œâ”€â”€ models/ # Pydantic schemas 

â”‚ â”œâ”€â”€ services/ # Core logic (chat, history, etc.) 

â”‚ â”œâ”€â”€ utils/ # Helpers (title generation) 

â”‚ â””â”€â”€ main.py # Entry point for FastAPI app 

â”‚
â”œâ”€â”€ frontend/ # HTML/JS Gemini-based frontend 

â”‚ â””â”€â”€ index.html 

â”‚
â”œâ”€â”€ .env # (NOT COMMITTED) MongoDB URI, LLM/Google API Key 

â”œâ”€â”€ requirements.txt # Python dependencies 

â””â”€â”€ run.py # Uvicorn launcher

---

## ğŸš€ Features

- ğŸ§  Chat with an LLM (e.g., OpenAI, Gemini, or local model)
- ğŸ’¬ Stores chat history in MongoDB
- ğŸ“š Fetch previous conversations with titles and timestamps
- ğŸŒ Simple frontend using `index.html` â€” no build step required
- ğŸ”’ Clean project architecture ready for production & extension

---

## âš™ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/fastapi-chatbot.git
cd fastapi-chatbot
```
### 2. Install Backend Dependencies
Use a virtual environment if possible. 
```bash 
pip install -r requirements.txt
```
### 3. Configure Environment Variables
Create a .env file in the root directory:
```bash
MONGODB_URI=mongodb+srv://<username>:<password>@cluster.mongodb.net/chatdb?retryWrites=true&w=majority
GOOGLE_API_KEY=your_gemini_or_llm_api_key
```
âš ï¸ Do not commit your .env file. It is excluded in .gitignore.

### 4. Run the FastAPI Backend
```bash
python run.py
```
Backend will be available at:
ğŸ“ http://127.0.0.1:8000

### 5. Run the Frontend
You can simply open frontend/index.html in your browser.

If your browser blocks local files, use Live Server in VS Code or run a simple local server:
```bash 
cd frontend
python -m http.server 8080
```
Then open:
ğŸ“ http://127.0.0.1:8080

### ğŸ” Notes
Ensure your MongoDB Atlas cluster is accessible from your IP.

This project assumes a Gemini-style or OpenAI-compatible ainvoke() method for LLMs.

Streaming support is planned â€” you can uncomment the provided stream_response() logic.

### ğŸ› ï¸ To-Do / Improvements

â³ Add streaming output support

ğŸ–¼ï¸ Add better frontend UI

âœ… LLM Memory / Context Awareness.


ğŸ“¦ Dockerize for full-stack deployment

### ğŸ¤ Contributing
Pull requests and improvements are welcome! 

If you spot bugs or have ideas for improvement, feel free to open an issue.